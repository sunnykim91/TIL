# CSR / SSR
## CSR - 클라이언트 사이드 렌더링
  - 순서
  - 1. 사용자가 웹사이트에 요청을 보낸다.
    2. CDN이 빠르게 HTML파일과 JS파일에 접근할 수 있는 링크를 보낸다.
    3. 브라우저는 HTML,JS 파일을 다운받고 그동안 화면에는 아무것도 보이지 않는다.
    4. 브라우저가 JS파일을 읽는다. (이때도 화면 안보임)
    5. 다운이 완료된 JS가 실행된다. 데이터를 위한 API가 호출된다.(이때 유저들은 placeholder를 보게된다.)
    6. 서버가 API 요청에 응답한다
    7. API로부터 받아온 data를 placeholder 자리에 넣어준다. 이제 페이지는 상호작용이 가능해진다.
    
  
## SSR- 서버 사이드 렌더링
  - 순서
  - 1. 사용자가 웹사이트에 요청을 보낸다
    2. 서버는 렌더링 가능한 HTML 파일을 만든다
    3. 브라우저는 빠르게 HTML파일을 읽을 수 있어 화면은 렌더링 된 상태이지만 아직 JS 파일을 읽지 않았기 때문에 조작은 불가능하다.
    4. 브라우저가 JS를 읽는다
    5. 이제 유저는 콘텐츠를 보고 사용자의 조작도 기록된다.
    6. 브라우저가 JS 프레임워크를 실행한다
    7. 기록된 사용자 조작도 실행되고 페이지 상호작용도 가능해진다.

## CSR vs SSR 비교

| 항목 | CSR (Client-Side Rendering) | SSR (Server-Side Rendering) |
|------|---------------------------|---------------------------|
| **렌더링 위치** | 브라우저(클라이언트)에서 JavaScript 실행 후 렌더링 | 서버에서 HTML을 미리 렌더링 후 전송 |
| **초기 로딩 속도** | 느림 (JavaScript 다운로드 및 실행 필요) | 빠름 (HTML이 즉시 제공됨) |
| **SEO (검색 엔진 최적화)** | 불리함 (초기 HTML이 비어 있어 검색 엔진이 크롤링하기 어려움) | 유리함 (완전한 HTML이 제공되므로 검색 엔진이 쉽게 크롤링 가능) |
| **페이지 전환 속도** | 빠름 (새로운 페이지를 불러오지 않고, 클라이언트에서 변경) | 느림 (페이지 이동 시마다 새로운 요청을 서버에 보내야 함) |
| **서버 부하** | 낮음 (서버는 정적인 HTML과 JavaScript만 제공) | 높음 (매 요청마다 새로운 HTML을 생성해야 함) |
| **유저 경험 (UX)** | 부드러운 SPA(Single Page Application) 가능 | 초기 로딩이 빠르지만 페이지 이동 시 깜빡임 가능 |
| **데이터 로딩 방식** | 클라이언트가 API 요청을 통해 데이터 가져옴 | 서버에서 데이터를 가져와 HTML과 함께 전달 |
| **예제 기술 스택** | React, Vue.js, Angular (Pure CSR) | Next.js, Nuxt.js, Express.js (SSR 지원) |

### 결론
- **CSR**은 SPA(Single Page Application)에 적합하며, 인터랙티브한 웹 애플리케이션에서 사용됨.
- **SSR**은 SEO가 중요한 블로그, 뉴스 사이트 등에 적합하며, 초기 로딩 속도가 빠름.
- 최근에는 **CSR과 SSR을 혼합한 하이브리드 방식** (Next.js 등)이 많이 사용됨.


### SSR-> CSR -> CSR+SSR, SSG
- 웹 초기 SSR을 기본적으로 사용하여 서버에서 HTML을 생성하여서 줬었음. 웹 어플리케이션의 발달에 따라 UX가 중요해졌는데, SSR은 페이지 전환시마다 새로고침되서 UX가 좋지않음 + 서버에서 계속 HTML 렌더해야하니까 부담이감
- Ajax(비동기 요청) 기술과 JavaScript 발전으로 클라이언트에서 동적 페이지 구현 가능해짐, SPA 개념 도입하여 처음에만 HTML을 불러오고 이후부터는 JavaScript가 데이터를 받아서 동적으로 화면을 변경 -> SEO문제, 초기로딩속도 문제발생
- Next.js의 등장 하이브리드 렌더링 - 최초 요청시에는 SSR로 이후에는 CSR로 / 초기에만 SSR로 속도를 높이고, SEO도 가능하게함
- SSG : 정적 사이트 생성 / 빌드 시 미리 HTML을 생성하여 저장하고, 요청이 오면 즉시 제공하는 방식 / 하지만 페이지를 미리 만들어두어야하므로 자주변경 되는 데이터엔 적용이 어렵다 / 주로 블로그, 문서 사이트 활용
    
### 크롤링과 SEO
- robots.txt는 검색 엔진이 크롤링할 수 있는 페이지와 크롤링하지 말아야 할 페이지를 지정하는 파일이며 최상위 디렉토리에 위치해야함
```
User-agent: *
Disallow: /admin/   # 관리자 페이지 크롤링 금지
Disallow: /private/ # 비공개 데이터 크롤링 금지
Allow: /
Sitemap: https://example.com/sitemap.xml
```

- SEO를 높이기 위한 노력들
  - 1. robots.txt, 사이트맵 설정
    2. 페이지 속도 최적화, 반응형으로 만들기
    3. 메타태그 최적화
    4. 시맨틱 태그 사용 - 검색엔진이 구조쉽게 파악가능, H1, H2 같은건 중요한 키워드로 인식하기도함
       
